{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+RP/l5ZfHaHhT3WMF9y3c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JulienHelfenstein/World_model/blob/main/05_run_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[box2d] numpy torch opencv-python tqdm pyvirtualdisplay xvfbwrapper &> /dev/null"
      ],
      "metadata": {
        "id": "4bwO0wvmT67-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0Iyv6RqOcmy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import gymnasium\n",
        "import numpy as np\n",
        "import os\n",
        "from time import sleep\n",
        "from google.colab import drive\n",
        "from pyvirtualdisplay import Display\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "from glob import glob\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "# Configurer l'affichage virtuel (nécessaire pour Colab)\n",
        "display_colab = Display(visible=0, size=(1400, 900))\n",
        "display_colab.start()\n",
        "\n",
        "# Monter Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ROOT = \"/content/drive/My Drive/Colab Notebooks/World_model\"\n",
        "VIDEO_DIR = os.path.join(PROJECT_ROOT, \"videos\")\n",
        "\n",
        "# S'assurer que le dossier vidéo existe\n",
        "if not os.path.exists(VIDEO_DIR):\n",
        "    os.makedirs(VIDEO_DIR)\n",
        "\n",
        "VAE_MODEL_PATH = os.path.join(PROJECT_ROOT, \"vae.pth\")\n",
        "RNN_MODEL_PATH = os.path.join(PROJECT_ROOT, \"rnn.pth\")\n",
        "CONTROLLER_SAVE_PATH = os.path.join(PROJECT_ROOT, \"controller.pth\")\n",
        "\n",
        "z_dim = 32\n",
        "action_dim = 3\n",
        "hidden_dim = 256\n",
        "num_mixtures = 5\n",
        "\n",
        "device = torch.device(\"cpu\") # CPU suffisant pour le run"
      ],
      "metadata": {
        "id": "zFVw7r5qOs9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CVAE(nn.Module):\n",
        "    def __init__(self, z_dim, image_channels=3):\n",
        "        super(CVAE, self).__init__()\n",
        "        self.z_dim = z_dim\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(image_channels, 32, 4, 2, 1), nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, 2, 1), nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, 2, 1), nn.ReLU(),\n",
        "            nn.Conv2d(128, 256, 4, 2, 1), nn.ReLU()\n",
        "        )\n",
        "        self.flat_size = 256 * 4 * 4\n",
        "        self.fc_mu = nn.Linear(self.flat_size, z_dim)\n",
        "        self.fc_logvar = nn.Linear(self.flat_size, z_dim)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x); h_flat = h.view(-1, self.flat_size)\n",
        "        return self.fc_mu(h_flat), self.fc_logvar(h_flat)\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(0.5 * log_var); eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "class MDNRNN(nn.Module):\n",
        "    def __init__(self, z_dim, action_dim, hidden_dim, num_mixtures):\n",
        "        super(MDNRNN, self).__init__()\n",
        "        self.z_dim = z_dim; self.hidden_dim = hidden_dim; self.num_mixtures = num_mixtures\n",
        "        input_dim = z_dim + action_dim\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "        mdn_output_dim = num_mixtures * (1 + 2 * z_dim)\n",
        "        self.mdn_output = nn.Linear(hidden_dim, mdn_output_dim)\n",
        "        self.reward_head = nn.Linear(hidden_dim, 1)\n",
        "        self.done_head = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, z_t, a_t, hidden_state):\n",
        "        lstm_input = torch.cat([z_t, a_t], dim=-1).unsqueeze(1)\n",
        "        lstm_output, next_hidden = self.lstm(lstm_input, hidden_state)\n",
        "        lstm_output = lstm_output.squeeze(1)\n",
        "        mdn_params = self.mdn_output(lstm_output)\n",
        "        pred_reward = self.reward_head(lstm_output)\n",
        "        pred_done_logits = self.done_head(lstm_output)\n",
        "        return mdn_params, pred_reward, pred_done_logits, next_hidden\n",
        "\n",
        "class Controller(nn.Module):\n",
        "    def __init__(self, z_dim, hidden_dim, action_dim):\n",
        "        super(Controller, self).__init__()\n",
        "        self.fc = nn.Linear(z_dim + hidden_dim, action_dim)\n",
        "\n",
        "    def forward(self, z_t, h_t):\n",
        "        action_unscaled = self.fc(torch.cat([z_t, h_t], dim=-1))\n",
        "        steer = torch.tanh(action_unscaled[:, 0:1])\n",
        "        gas = torch.sigmoid(action_unscaled[:, 1:2])\n",
        "        brake = torch.sigmoid(action_unscaled[:, 2:3])\n",
        "        return torch.cat([steer, gas, brake], dim=-1)"
      ],
      "metadata": {
        "id": "HSqxeS6lOrNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_obs(obs):\n",
        "    obs_tensor = torch.from_numpy(obs).permute(2, 0, 1).float() / 255.0\n",
        "    obs_tensor = obs_tensor.unsqueeze(0).to(device)\n",
        "    obs_resized = F.interpolate(obs_tensor, size=(64, 64), mode='bilinear', align_corners=False)\n",
        "    return obs_resized"
      ],
      "metadata": {
        "id": "dL4iUFO6OojW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_agent():\n",
        "    print(\"Chargement des modèles entraînés...\")\n",
        "\n",
        "    vae = CVAE(z_dim).to(device); vae.load_state_dict(torch.load(VAE_MODEL_PATH, map_location=device)); vae.eval()\n",
        "    rnn = MDNRNN(z_dim, action_dim, hidden_dim, num_mixtures).to(device); rnn.load_state_dict(torch.load(RNN_MODEL_PATH, map_location=device)); rnn.eval()\n",
        "    controller = Controller(z_dim, hidden_dim, action_dim).to(device); controller.load_state_dict(torch.load(CONTROLLER_SAVE_PATH, map_location=device)); controller.eval()\n",
        "\n",
        "    print(\"Modèles chargés. Lancement de l'environnement...\")\n",
        "\n",
        "    # --- MODIFICATION POUR LA VIDÉO ---\n",
        "    # 1. Utiliser 'rgb_array' au lieu de 'human'\n",
        "    env = gymnasium.make('CarRacing-v2', render_mode='rgb_array')\n",
        "    # 2. Envelopper l'environnement pour enregistrer une vidéo\n",
        "    # (On enregistre seulement le premier épisode pour l'exemple)\n",
        "    env = RecordVideo(env, video_folder=VIDEO_DIR, episode_trigger=lambda e: e == 0)\n",
        "\n",
        "    for episode in range(2): # Lancer 2 épisodes (seul le 1er sera enregistré)\n",
        "        print(f\"--- Début de l'Épisode {episode + 1} ---\")\n",
        "        total_reward = 0\n",
        "        obs, _ = env.reset()\n",
        "        h_t = torch.zeros(1, hidden_dim).to(device)\n",
        "        c_t = torch.zeros(1, hidden_dim).to(device)\n",
        "\n",
        "        while True:\n",
        "            with torch.no_grad():\n",
        "                obs_preprocessed = preprocess_obs(obs)\n",
        "                z_t = vae.reparameterize(*vae.encode(obs_preprocessed))\n",
        "                action_tensor = controller(z_t, h_t)\n",
        "                _, _, _, (h_t, c_t) = rnn(z_t, action_tensor, (h_t.unsqueeze(0), c_t.unsqueeze(0)))\n",
        "                h_t, c_t = h_t.squeeze(0), c_t.squeeze(0)\n",
        "\n",
        "            action_np = action_tensor.squeeze(0).cpu().numpy()\n",
        "            obs, reward, terminated, truncated, _ = env.step(action_np)\n",
        "            total_reward += reward\n",
        "\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "        print(f\"Épisode terminé. Récompense totale : {total_reward:.2f}\")\n",
        "\n",
        "    env.close()\n",
        "    print(\"Simulation terminée.\")\n",
        "\n",
        "    # --- 6. Afficher la vidéo dans Colab ---\n",
        "    print(\"Affichage de la vidéo enregistrée...\")\n",
        "    video_files = glob(os.path.join(VIDEO_DIR, \"*.mp4\"))\n",
        "\n",
        "    if video_files:\n",
        "        video_path = sorted(video_files)[-1] # Prendre la vidéo la plus récente\n",
        "        html = f\"\"\"\n",
        "        <video width=\"600\" controls>\n",
        "          <source src=\"{video_path}\" type=\"video/mp4\">\n",
        "        </video>\n",
        "        \"\"\"\n",
        "        display(HTML(html))\n",
        "        print(f\"Vidéo affichée depuis {video_path}\")\n",
        "    else:\n",
        "        print(\"Aucun fichier vidéo trouvé.\")\n",
        "\n",
        "# --- Lancer le tout ---\n",
        "if __name__ == \"__main__\":\n",
        "    run_agent()"
      ],
      "metadata": {
        "id": "NjmPyqYEOm5F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}