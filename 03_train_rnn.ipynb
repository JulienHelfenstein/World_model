{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOEPlOZnXMZhcWw71z1pRBB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JulienHelfenstein/World_model/blob/main/03_train_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Définir le chemin racine de votre projet\n",
        "PROJECT_ROOT = \"/content/drive/My Drive/Colab Notebooks/World_model\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyWAXTPNTk4C",
        "outputId": "22e978de-38a6-4669-9619-284c3c06dc50"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "V3Rlre33M8db"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from torch.distributions import Categorical, Normal, MixtureSameFamily, Independent"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Configuration et Hyperparamètres ---\n",
        "VAE_MODEL_PATH = os.path.join(PROJECT_ROOT, \"vae.pth\")\n",
        "DATA_FILE = os.path.join(PROJECT_ROOT, \"data/carracing_data.npz\")\n",
        "RNN_DATA_FILE = os.path.join(PROJECT_ROOT, \"data/rnn_data.npz\")\n",
        "RNN_MODEL_PATH = os.path.join(PROJECT_ROOT, \"rnn.pth\")\n",
        "\n",
        "# Paramètres (doivent correspondre au VAE et aux données)\n",
        "z_dim = 32\n",
        "action_dim = 3  # CarRacing: [steer, gas, brake]\n",
        "hidden_dim = 256 # Taille de la mémoire du LSTM\n",
        "num_mixtures = 5 # Nombre de \"futurs\" possibles à prédire\n",
        "seq_length = 50  # Longueur des séquences pour l'entraînement du RNN\n",
        "batch_size = 32\n",
        "learning_rate = 1e-3\n",
        "num_epochs = 10  # 10-20 époques est un bon début\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "tblkqomHNZRc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Définition des Modèles ---\n",
        "\n",
        "# On a besoin de la *définition COMPLÈTE* du CVAE\n",
        "class CVAE(nn.Module):\n",
        "    def __init__(self, z_dim, image_channels=3):\n",
        "        super(CVAE, self).__init__()\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # --- Encodeur (Image -> Espace Latent) ---\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(image_channels, 32, 4, 2, 1), nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, 2, 1), nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, 2, 1), nn.ReLU(),\n",
        "            nn.Conv2d(128, 256, 4, 2, 1), nn.ReLU()\n",
        "        )\n",
        "        self.flat_size = 256 * 4 * 4\n",
        "        self.fc_mu = nn.Linear(self.flat_size, z_dim)\n",
        "        self.fc_logvar = nn.Linear(self.flat_size, z_dim)\n",
        "\n",
        "        # --- DÉCODEUR (DOIT ÊTRE PRÉSENT POUR LE CHARGEMENT) ---\n",
        "        self.decoder_fc = nn.Linear(z_dim, self.flat_size)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, 2, 1), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, image_channels, 4, 2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h_flat = h.reshape(-1, self.flat_size) # <-- Ligne corrigée\n",
        "        return self.fc_mu(h_flat), self.fc_logvar(h_flat)\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(0.5 * log_var); eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    # --- AJOUTER LA FONCTION DECODE ---\n",
        "    def decode(self, z):\n",
        "        h = F.relu(self.decoder_fc(z))\n",
        "        h_unflat = h.view(-1, 256, 4, 4)\n",
        "        return self.decoder(h_unflat)\n",
        "\n",
        "\n",
        "# Le \"Moteur de Rêve\" (MDN-RNN)\n",
        "\n",
        "class MDNRNN(nn.Module):\n",
        "    def __init__(self, z_dim, action_dim, hidden_dim, num_mixtures):\n",
        "        super(MDNRNN, self).__init__()\n",
        "        self.z_dim = z_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_mixtures = num_mixtures\n",
        "\n",
        "        input_dim = z_dim + action_dim\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "        mdn_output_dim = num_mixtures * (1 + 2 * z_dim)\n",
        "        self.mdn_output = nn.Linear(hidden_dim, mdn_output_dim)\n",
        "\n",
        "    def forward(self, z_seq, a_seq, hidden_state):\n",
        "        # z_seq shape: (batch_size, seq_len, z_dim)\n",
        "        # a_seq shape: (batch_size, seq_len, action_dim)\n",
        "        lstm_input = torch.cat([z_seq, a_seq], dim=-1)\n",
        "\n",
        "        # lstm_output shape: (batch_size, seq_len, hidden_dim)\n",
        "        lstm_output, next_hidden = self.lstm(lstm_input, hidden_state)\n",
        "\n",
        "        # mdn_params shape: (batch_size, seq_len, mdn_output_dim)\n",
        "        mdn_params = self.mdn_output(lstm_output)\n",
        "\n",
        "        distribution = self.get_mixture_distribution(mdn_params)\n",
        "        return distribution, next_hidden\n",
        "\n",
        "    def get_mixture_distribution(self, mdn_params):\n",
        "        \"\"\"\n",
        "        Prend la sortie brute du réseau et la transforme en une\n",
        "        distribution de probabilité PyTorch.\n",
        "        \"\"\"\n",
        "\n",
        "        # --- PARTIE MANQUANTE ---\n",
        "        # Sépare la sortie brute en pi, mu, sigma\n",
        "\n",
        "        # pi_logits shape: (batch_size, seq_len, num_mixtures)\n",
        "        pi_logits = mdn_params[..., :self.num_mixtures]\n",
        "\n",
        "        # mu shape: (batch_size, seq_len, num_mixtures, z_dim)\n",
        "        mu = mdn_params[..., self.num_mixtures : self.num_mixtures * (1 + self.z_dim)]\n",
        "        mu = mu.view(mdn_params.size(0), mdn_params.size(1), self.num_mixtures, self.z_dim)\n",
        "\n",
        "        # log_sigma shape: (batch_size, seq_len, num_mixtures, z_dim)\n",
        "        log_sigma = mdn_params[..., self.num_mixtures * (1 + self.z_dim) :]\n",
        "        log_sigma = log_sigma.view(mdn_params.size(0), mdn_params.size(1), self.num_mixtures, self.z_dim)\n",
        "        # --- FIN DE LA PARTIE MANQUANTE ---\n",
        "\n",
        "\n",
        "        # --- CORRECTIF PRÉCÉDENT ---\n",
        "        pi_dist = Categorical(logits=pi_logits)\n",
        "        sigma = torch.exp(log_sigma) + 1e-6\n",
        "\n",
        "        base_distribution = Normal(loc=mu, scale=sigma)\n",
        "\n",
        "        # On dit à PyTorch que la dernière dimension (de taille z_dim)\n",
        "        # fait partie de l'événement, pas du batch.\n",
        "        gaussian_dist = Independent(base_distribution, 1)\n",
        "        # --- FIN DU CORRECTIF PRÉCÉDENT ---\n",
        "\n",
        "        mixture_dist = MixtureSameFamily(pi_dist, gaussian_dist)\n",
        "        return mixture_dist"
      ],
      "metadata": {
        "id": "xTW29h5nNWsU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Fonction de Perte (Loss) du MDN ---\n",
        "def mdn_loss_function(mixture_distribution, target_z):\n",
        "    \"\"\"\n",
        "    Calcule la perte (log-probabilité négative).\n",
        "\n",
        "    target_z shape est [batch, seq_len, z_dim]\n",
        "    mixture_distribution.batch_shape est [batch, seq_len]\n",
        "    mixture_distribution.event_shape est [z_dim]\n",
        "\n",
        "    Les formes sont maintenant directement compatibles !\n",
        "    \"\"\"\n",
        "    log_prob = mixture_distribution.log_prob(target_z)\n",
        "\n",
        "    # On veut maximiser la log-probabilité, donc on minimise son opposé\n",
        "    return -torch.mean(log_prob)"
      ],
      "metadata": {
        "id": "P5N2QbixNTRF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Phase 2a: Pré-traitement des données (VAE -> Z) ---\n",
        "def create_rnn_data():\n",
        "    if os.path.exists(RNN_DATA_FILE):\n",
        "        print(f\"Le fichier de données pré-traitées {RNN_DATA_FILE} existe déjà.\")\n",
        "        return\n",
        "\n",
        "    print(\"Phase 2a: Pré-traitement des données (Images -> Vecteurs Z)...\")\n",
        "\n",
        "    # 1. Charger le VAE entraîné\n",
        "    vae_model = CVAE(z_dim).to(device)\n",
        "    vae_model.load_state_dict(torch.load(VAE_MODEL_PATH, map_location=device))\n",
        "    vae_model.eval() # Mode évaluation (gèle les poids)\n",
        "\n",
        "    # 2. Charger les données brutes (images + actions)\n",
        "    raw_data = np.load(DATA_FILE)\n",
        "    observations = raw_data['observations'] # shape (N, 64, 64, 3)\n",
        "    actions = raw_data['actions']           # shape (N, 3)\n",
        "\n",
        "    # 3. Transformer les images en tenseurs (N, C, H, W)\n",
        "    obs_tensor = torch.from_numpy(observations).permute(0, 3, 1, 2).to(device, dtype=torch.float32)\n",
        "\n",
        "    z_vectors = []\n",
        "\n",
        "    # 4. Encoder toutes les images en vecteurs Z\n",
        "    # On traite par petits batchs pour ne pas saturer la VRAM\n",
        "    vae_batch_size = 256\n",
        "    with torch.no_grad(): # TRES IMPORTANT: pas de calcul de gradient\n",
        "        pbar = tqdm(range(0, len(obs_tensor), vae_batch_size), desc=\"Encodage VAE\")\n",
        "        for i in pbar:\n",
        "            batch_obs = obs_tensor[i : i + vae_batch_size]\n",
        "            mu, log_var = vae_model.encode(batch_obs)\n",
        "            z = vae_model.reparameterize(mu, log_var)\n",
        "            z_vectors.append(z.cpu().numpy()) # Stocker sur CPU\n",
        "\n",
        "    # 5. Concaténer et sauvegarder\n",
        "    all_z = np.concatenate(z_vectors, axis=0)\n",
        "    all_actions = actions # Les actions n'ont pas besoin de changer\n",
        "\n",
        "    print(f\"Encodage terminé. Shape Z: {all_z.shape}, Shape A: {all_actions.shape}\")\n",
        "    np.savez_compressed(RNN_DATA_FILE, z_vectors=all_z, actions=all_actions)\n",
        "    print(f\"Données pré-traitées sauvegardées dans {RNN_DATA_FILE}\")"
      ],
      "metadata": {
        "id": "4kfxMIlfNRe0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Phase 2b: Dataset pour Séquences ---\n",
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, data_file, seq_length):\n",
        "        data = np.load(data_file)\n",
        "        self.z_vectors = torch.from_numpy(data['z_vectors']).float()\n",
        "        self.actions = torch.from_numpy(data['actions']).float()\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        # On ne peut pas commencer une séquence près de la fin\n",
        "        return len(self.z_vectors) - self.seq_length - 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Séquence d'entrée\n",
        "        z_seq = self.z_vectors[idx : idx + self.seq_length]\n",
        "        a_seq = self.actions[idx : idx + self.seq_length]\n",
        "\n",
        "        # Séquence cible (décalée d'un pas dans le temps)\n",
        "        target_z_seq = self.z_vectors[idx + 1 : idx + self.seq_length + 1]\n",
        "\n",
        "        return z_seq, a_seq, target_z_seq"
      ],
      "metadata": {
        "id": "1XtKAz49NPEd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. Script Principal d'Entraînement (Phase 2b) ---\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # 1. Lancer la phase 2a (pré-traitement)\n",
        "    create_rnn_data()\n",
        "\n",
        "    print(\"Phase 2b: Entraînement du MDN-RNN...\")\n",
        "\n",
        "    # 2. Créer le Dataset et le DataLoader\n",
        "    dataset = SequenceDataset(RNN_DATA_FILE, seq_length)\n",
        "    data_loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    # 3. Initialiser le Modèle et l'Optimiseur\n",
        "    model = MDNRNN(z_dim, action_dim, hidden_dim, num_mixtures).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    model.train() # Mode entraînement\n",
        "\n",
        "    # 4. Boucle d'Entraînement\n",
        "    for epoch in range(num_epochs):\n",
        "        pbar = tqdm(data_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        total_epoch_loss = 0\n",
        "\n",
        "        for z_seq, a_seq, target_z_seq in pbar:\n",
        "            z_seq = z_seq.to(device)\n",
        "            a_seq = a_seq.to(device)\n",
        "            target_z_seq = target_z_seq.to(device)\n",
        "\n",
        "            # Initialiser l'état caché (h, c) pour le LSTM\n",
        "            hidden_state = (torch.zeros(1, batch_size, hidden_dim).to(device),\n",
        "                            torch.zeros(1, batch_size, hidden_dim).to(device))\n",
        "\n",
        "            # --- Forward pass ---\n",
        "            # Gérer le dernier batch (qui peut être plus petit)\n",
        "            if z_seq.size(0) != batch_size:\n",
        "                hidden_state = (torch.zeros(1, z_seq.size(0), hidden_dim).to(device),\n",
        "                                torch.zeros(1, z_seq.size(0), hidden_dim).to(device))\n",
        "\n",
        "            distribution, _ = model(z_seq, a_seq, hidden_state)\n",
        "\n",
        "            # --- Calcul de la perte ---\n",
        "            loss = mdn_loss_function(distribution, target_z_seq)\n",
        "\n",
        "            # --- Backward pass ---\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_epoch_loss += loss.item()\n",
        "            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "\n",
        "        avg_loss = total_epoch_loss / len(data_loader)\n",
        "        print(f\"Fin Epoch {epoch+1}. Perte moyenne : {avg_loss:.4f}\")\n",
        "\n",
        "    # 5. Sauvegarder le modèle\n",
        "    print(\"Entraînement du RNN terminé.\")\n",
        "    torch.save(model.state_dict(), RNN_MODEL_PATH)\n",
        "    print(f\"Modèle RNN sauvegardé dans {RNN_MODEL_PATH}\")"
      ],
      "metadata": {
        "id": "WQM059OLNM3V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c54c42d0-7b73-4ef0-da03-35f4e57197f9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le fichier de données pré-traitées /content/drive/My Drive/Colab Notebooks/World_model/data/rnn_data.npz existe déjà.\n",
            "Phase 2b: Entraînement du MDN-RNN...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 100%|██████████| 933/933 [00:09<00:00, 99.19it/s, loss=24.9347] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fin Epoch 1. Perte moyenne : 26.7457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|██████████| 933/933 [00:09<00:00, 98.47it/s, loss=19.8044] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fin Epoch 2. Perte moyenne : 22.7900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|██████████| 933/933 [00:09<00:00, 100.22it/s, loss=17.2101]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fin Epoch 3. Perte moyenne : 20.9248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|██████████| 933/933 [00:08<00:00, 110.53it/s, loss=20.1053]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fin Epoch 4. Perte moyenne : 19.5993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|██████████| 933/933 [00:09<00:00, 100.14it/s, loss=15.4963]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fin Epoch 5. Perte moyenne : 18.5970\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|██████████| 933/933 [00:09<00:00, 99.32it/s, loss=20.3196] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fin Epoch 6. Perte moyenne : 17.8368\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|██████████| 933/933 [00:08<00:00, 110.51it/s, loss=16.6992]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fin Epoch 7. Perte moyenne : 17.2057\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|██████████| 933/933 [00:09<00:00, 100.24it/s, loss=15.8183]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fin Epoch 8. Perte moyenne : 16.6984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|██████████| 933/933 [00:11<00:00, 78.23it/s, loss=14.7599] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fin Epoch 9. Perte moyenne : 16.2555\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|██████████| 933/933 [00:09<00:00, 103.60it/s, loss=16.9940]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fin Epoch 10. Perte moyenne : 15.9001\n",
            "Entraînement du RNN terminé.\n",
            "Modèle RNN sauvegardé dans /content/drive/My Drive/Colab Notebooks/World_model/rnn.pth\n"
          ]
        }
      ]
    }
  ]
}